{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a5517d7993b4b35049f0013dd6a3f55",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','7'), \"You are not running Python 3.7. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5285d7fc7207e47d532f63ff1fb7a339",
     "grade": false,
     "grade_id": "cell-3c556a39514d3d6c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HA2:  Part 2 - Transformers and self-attention\n",
    "$$\n",
    "\\renewcommand{\\vec}[1]{#1}\n",
    "\\def\\x{\\vec{x}}\n",
    "\\def\\y{\\vec{y}}\n",
    "\\def\\dim{d}\n",
    "\\def\\w{W}\n",
    "\\def\\wu{Z}\n",
    "\\def\\R{\\mathbb{R}}\n",
    "\\def\\linMap{W}\n",
    "% Query, key and val\n",
    "\\def\\q{\\vec{q}}\n",
    "\\def\\k{\\vec{k}}\n",
    "\\def\\v{\\vec{v}}\n",
    "\\def\\Wq{\\linMap_Q}\n",
    "\\def\\Wk{\\linMap_K}\n",
    "\\def\\Wv{\\linMap_V}\n",
    "$$\n",
    "*You should have completed part 1 before starting with this one*\n",
    "\n",
    "In this part we will take a closer look at the transformer architecture and the self-attention operation.\n",
    "We will start with basic self-attention and gradually construct an actual self-attention module.\n",
    "Finally we will construct a complete transformer and test it on an actual problem.\n",
    "\n",
    "The focus is on a conceptual understanding of the transformer but you will have to implement a few key elements of a transformer. Along the way we will try to give some best practices for constructing a more complex network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e561aaa14e57e43514a3169d5142c9cb",
     "grade": false,
     "grade_id": "cell-1531dbb9354dac88",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's start with importing the module's we are going to need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "809176a963532ded6fab7314251a8556",
     "grade": false,
     "grade_id": "cell-a4bff0f2271fff88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as torch_nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic self-attention\n",
    "\n",
    "The key-stone of the transformer architecture, self-attention is a sequence-sequence operation which transforms a sequence of input vectors $\\x_1, \\dots \\x_t$ to output vectors $\\y_1, \\dots \\y_t$.\n",
    "Remember that all vectors have the same dimension $\\dim$, i.e. $\\x_i, \\y_i \\in \\R^{\\dim}, \\forall i = 1, \\dots t$.\n",
    "\n",
    "## Weighted average\n",
    "The actual transformation is a simple weighted average\n",
    "$$\n",
    "\\y_i = \\sum_{j} \\x_j \\w_{ji}.\n",
    "$$\n",
    "\n",
    "In an actual transformer, weighted averages are computed often and for long sequences. Therefore, the implementation must be fast in order for training to be even possible.\n",
    "With high-level frameworks such as `pytorch`, the key to fast code is often to reduce loops and instead express computations as matrix operations.\n",
    "\n",
    "**(2 POE)** Complete the function snippet below to implement simple weight sharing.\n",
    "\n",
    "To pass this part of the assignment your implementation only has to be correct, not efficient, but to get the first POE, you must implement it with just a single for loop. For the second POE, do it without any loops at all.\n",
    "\n",
    "*Hint*: Take a look at how `torch.bmm` is used later in the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f9fc770ac37f0d03e737a5eefa8705c",
     "grade": true,
     "grade_id": "cell-f846c494c826a310",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_avg(x, weights):\n",
    "    \"\"\"Weighted average\n",
    "    Calculates a weighted average of a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        weights (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        y (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return torch.bmm(x,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e9dc459927bdd7a1c2ca05c946e71aa",
     "grade": false,
     "grade_id": "cell-c2fb9d37d6959f02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure to test your implementation with the unit tests below.\n",
    "The tests cover:\n",
    "\n",
    "1. Dimensionality\n",
    "2. Uniform weights $\\w_{ji} = \\frac{1}{t}$ should produce $y_i: y_i = \\frac{1}{\\dim} \\sum_{j} x_j,\\, \\forall i = 1, \\dots t$\n",
    " (i.e., every $y_i$ is an average of the input sequence).\n",
    "3. A specific numerical example with batch size = 2, $t = 2,\\, \\dim=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "854fbde40a06070cdb6d40423c9f49a1",
     "grade": false,
     "grade_id": "cell-1a58d8ec5834578f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "def test_weighted_avg(function):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        function: Implementation to test\n",
    "    \"\"\"\n",
    "    # Testing dimension of averaged tensor.\n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    x = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = torch.rand(batch_size, seq_len, seq_len)\n",
    "    y = function(x, weights)\n",
    "    assert y.shape == (batch_size, dim, seq_len), \"Dimension error: expected y to have shape {}, got {}.\".format(\n",
    "        (batch_size, seq_len, dim), tuple(y.shape))\n",
    "    \n",
    "    # Testing uniform weights preserve x.\n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    x = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = torch.ones((batch_size, seq_len, seq_len)).float() / seq_len\n",
    "    y = function(x, weights)\n",
    "    assert all(torch.allclose(y_b.mean(1), y_b[:, 0]) for y_b in y),\\\n",
    "        \"Numerical error: With uniform weights, expected y_i = y_j forall i, j (within each batch).\"\n",
    "    assert all(torch.allclose(y_b.mean(1), x_b.mean(1)) for (x_b, y_b) in zip(x, y)),\\\n",
    "        \"Numerical error: With uniform weights, expected y_i = sum_j x_j, for all i\"\n",
    "    \n",
    "    # Actual numerical example.\n",
    "    x = torch.tensor([4, 1]).reshape((1, 1, 2)).float()\n",
    "    unnorm_weights = torch.arange(1, 5).reshape((1, 2, 2)).float()\n",
    "    scale = unnorm_weights.sum(1).reshape((1, 1, 2))\n",
    "    weights = unnorm_weights / scale\n",
    "\n",
    "    y = function(x, weights)\n",
    "    y_true = torch.tensor([7/4, 2]).reshape(1, 1, 2).float()\n",
    "    assert torch.allclose(y, y_true), \"Numerical error, expected: {}, got {}\".format(y_true, y)\n",
    "    \n",
    "    print(\"Test passed.\")\n",
    "\n",
    "test_weighted_avg(function=weighted_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9af181fb1e50970e08d65e6da4f51c1f",
     "grade": false,
     "grade_id": "cell-06c2f57520370ab9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Defining weights through the dot product\n",
    "A simple way to define $\\w_{ji}$ is with the dot product\n",
    "\n",
    "$$\n",
    "\\wu_{ji} = \\x_j^T \\x_i.\n",
    "$$\n",
    "which maps the pair of input vectors to a non-negative scalar, $\\R^{\\dim \\times \\dim} \\to [0, \\infty)$.\n",
    "We then use a softmax to obtain normalised $\\w_{ji} \\in (0, 1]$:\n",
    "\n",
    "$$\n",
    "\\w_{ji} = \\frac{ e^{\\wu_{ji}} }{ \\sum_j e^{\\wu_{ji}} }.\n",
    "$$\n",
    "\n",
    "**(1 POE)**\n",
    "What is the difference between these weights and the weights in ordinary networks, e.g. a CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b5c80f2d75dcacf639bb3102a94456e",
     "grade": false,
     "grade_id": "cell-39d421f43d0c02c4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** \n",
    "The dot product is essential for calculating the weights. As we progress, we will make slight modifications to the inputs but we will still base it around a function which calculates a softmax-normalized dot product.\n",
    "Therefore, you need to complete the implementation below:\n",
    "\n",
    "Again, this function will be evaluated often and for long sequences in the transformer block. For POE's, implement it without using for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0d24b818f400320a93a31e22555ed14",
     "grade": true,
     "grade_id": "cell-2a548cd9fdb03d8f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalized_dot_product(v_1, v_2):\n",
    "    \"\"\"Normalized dot products between all pairs of vectors in a sequence\n",
    "    Takes two batches of sequences of vectors as input.\n",
    "    Sequences in the batch are processed independently.\n",
    "    The normalization is done with a softmax function along the columns of the weight matrices.\n",
    "    \n",
    "    Args:\n",
    "        v_1 (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        v_2 (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        norm_dot_prod (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    xj=v_1.transpose(1,2)\n",
    "    xi=v_2\n",
    "    z=torch.bmm(xj,xi)\n",
    "    output=F.softmax(z,dim=1)\n",
    "    # YOUR CODE HERE\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "609f29304963a4e99b92f240233ba272",
     "grade": false,
     "grade_id": "cell-9a0498b4a61bc45e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Make sure to test your implementation with the unit tests below.\n",
    "The tests cover:\n",
    "\n",
    "1. Dimensionality\n",
    "2. Normalized in the correct dimension\n",
    "3. A specific numerical example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab2cff4a1da3ac7edec034bce652b746",
     "grade": false,
     "grade_id": "cell-17aa1999bb0519d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_normalized_dot_product(function):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        function: Implementation to test\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size, dim, seq_len = 5, 2, 3\n",
    "    v_1 = torch.rand(batch_size, dim, seq_len)\n",
    "    v_2 = torch.rand(batch_size, dim, seq_len)\n",
    "    weights = function(v_1, v_2)\n",
    "    \n",
    "    # Testing dimension of weights.\n",
    "    assert weights.shape == (batch_size, seq_len, seq_len),\\\n",
    "    \"Dimension error: expected weights to have shape {}, got {}.\".format(\n",
    "        (batch_size, seq_len, seq_len), tuple(weights.shape))\n",
    "    \n",
    "    # Testing weights non-negative\n",
    "    # (Boolean tensor's can be reduced to a single boolean)\n",
    "    assert not (weights < 0.0).any() ,\\\n",
    "    \"Value error: expected weights to be non-negative.\"\n",
    "    \n",
    "    # Testing weights smaller than one\n",
    "    assert (weights < 1.0).all() ,\\\n",
    "    \"Value error: expected weights to be non-negative.\"\n",
    "    \n",
    "    assert torch.allclose(weights.sum(1), torch.ones((batch_size, seq_len))),\\\n",
    "        \"ValueError: expected columns (dim 1) to sum to 1.0\"\n",
    "    \n",
    "    # Actual numerical example\n",
    "    v_1 = torch.tensor([[1, 2], [-1, 1]]).float().reshape((1, 2, 2))\n",
    "    v_2 = torch.tensor([[1, 0], [1, -1]]).float().reshape((1, 2, 2))\n",
    "    e = np.exp(1)\n",
    "    true_weights = torch.tensor([\n",
    "        [1 / (e**3 + 1), e**2 / (e**2 + 1)],\n",
    "        [e**3 / (e**3 + 1), 1 / (e**2 + 1)]\n",
    "    ]).reshape((1, 2, 2))\n",
    "    weights =  function(v_1, v_2)\n",
    "    assert torch.allclose(true_weights, weights),\\\n",
    "    \"Numerical error: expected {}, got {}.\".format(true_weights, weights)\n",
    "    \n",
    "    print(\"Test passed.\")   \n",
    "    \n",
    "test_normalized_dot_product(function=normalized_dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bea90e7cfd9acf3023bd00744e551f9a",
     "grade": false,
     "grade_id": "cell-746bafddcbfd9aba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "That's it, we have now the building blocks needed for basic self-attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d52434db03615e9fe1e3c6ff3070a472",
     "grade": false,
     "grade_id": "cell-4bb35eab7404cf63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def basic_self_attention(x):\n",
    "    \"\"\"Basic self-attention\n",
    "    Transforms a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        y (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \"\"\"\n",
    "    weights = normalized_dot_product(x, x)\n",
    "    return weighted_avg(x, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb6a1b8ddffbfb578599f99b30ef658d",
     "grade": false,
     "grade_id": "cell-648a50a17d0269e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 2. A self-attention module\n",
    "Like you saw in the video lectures, self-attention is rarely used in the basic form we have created above.\n",
    "Let's do the modifications needed to construct an actual transformer.\n",
    "\n",
    "We will wrap it in a proper `torch.nn` module to create a building block that we can use in a network.\n",
    "Creating your own module is actually not that common, frameworks like `pytorch` are built to be *modular* and we can often create very specific networks by combining standard modules. That is a good thing, since it enables us to express interesting models in a high-level interface and as a bonus, we build a model from well-tested and efficient parts.\n",
    "With that said, you might find yourself in a situation (perhaps already in the project) where no off-the-shelf module suits your need and you have to create one yourself. View this latter part as an example/inspiration of how to construct a non-trivial custom module.\n",
    "\n",
    "## Queries, keys and values\n",
    "The self-attention is extended with three linear mappings $\\Wq, \\Wk, \\Wv \\in \\R^{\\dim \\times \\dim}$ .\n",
    "These give us learnable parameters and make self-attention more flexible.\n",
    "The three matrices map the input $\\x_i$ into a query, key and value respectively:\n",
    "\n",
    "\\begin{align}\n",
    "    \\q_i = \\Wq \\x_i \\\\\n",
    "    \\k_i = \\Wk \\x_i \\\\\n",
    "    \\v_i = \\Wv \\x_i\n",
    "\\end{align}\n",
    "\n",
    "First, we modify the self-attention by redefining the unnormalized weights (while reusing the notation):\n",
    "\n",
    "\\begin{align}\n",
    "    \\wu_{ji} = \\q_j^T \\k_i \\Big{/} \\sqrt{\\dim}\n",
    "\\end{align}\n",
    "The normalized weights are still obtained by applying the softmax function.\n",
    "\n",
    "**(1 POE)** Explain why we scale the dot product with the factor $1 / \\sqrt{\\dim}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bf491e786332986bb598f8e10a442c2",
     "grade": true,
     "grade_id": "cell-d351e9c8c79ee5df",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** We scale it because, we need to avoid the effeccts of steep gradient that occurs while training the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d765a71cc1bfa1048bc4d2c664d1b402",
     "grade": false,
     "grade_id": "cell-eeac94c3ecfcc5e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, the weighted average modified and is now based on the values $\\v_j$, instead of on $\\x_j$ directly:\n",
    "$$\n",
    "\\y_i = \\sum_{j} \\v_j \\w_{ji}.\n",
    "$$\n",
    "\n",
    "We can reuse our dot product calculation by simple *wrapping* it in a function that takes queries and keys as the argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a73b126acb49c060f096918bdb48cf5",
     "grade": false,
     "grade_id": "cell-e1459474d3e61f00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def query_key_weights(queries, keys):\n",
    "    \"\"\"Weights from query-key dot product.\n",
    "    Softmax-normalised dot product weights\n",
    "    Calculates weights for a batch of sequences of vectors.\n",
    "    \n",
    "    Args:\n",
    "        queries (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "        keys (torch.Tensor): Shape (batch_size, dim, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        weights (torch.Tensor): Shape (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    dim = queries.shape[2]\n",
    "    queries = queries / (dim ** (1/4))\n",
    "    keys    = keys / (dim ** (1/4))\n",
    "    return normalized_dot_product(queries, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c6a63e908bd4a4bb842cd976e42cf63",
     "grade": false,
     "grade_id": "cell-4473215afd0827d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Multi-head self-attention\n",
    "\n",
    "The model should be able to find different patterns in the input sequence, which is why we use multiple heads.\n",
    "\n",
    "Now, we'll create the actual self-attention function, which includes multiple heads.\n",
    "For implementation simplicity and efficiency we will do a version called *narrow* self-attention, where the input vector is split into parts and each attention head is applied to just one part of the vector.\n",
    "Imagine that we have $\\d = 64$ and four heads, then each head would operate on a vector with dimension $64 / 4 = 16$.\n",
    "\n",
    "## Constructing the module\n",
    "Below is an implementation of our self-attention module. We try to show you how a typical custom model looks like. Part of that is to do full vectorization (i.e. no loops). The result is a lot of manipulation of shapes and dimension order of intermediate tensors. It is not very readable and it is quite difficult to wrap your head around it but since you are likely to use and modify other peoples code (in the project or some later time), it is good that you get exposed to it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ace34fbf2dc8bbb3b664159fe0b5cd2",
     "grade": false,
     "grade_id": "cell-535e46dd571b9284",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(torch_nn.Module):\n",
    "    def __init__(self, dim, heads):\n",
    "        \"\"\"(Narrow) Self-attention module\n",
    "\n",
    "        Args:\n",
    "            dim (int): The full embedding dimension of the input vectors\n",
    "            heads (int): The number of heads in the multi-head attention.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not dim % heads == 0:\n",
    "            raise ValueError(\n",
    "                \"The embedding dim. must be divisible by the number of heads for the vectorization to work.\"\n",
    "            )\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        part_dim = dim // heads\n",
    "        # Linear maps for q, k and v\n",
    "        self.Wq = torch_nn.Linear(part_dim, part_dim, bias=False)\n",
    "        self.Wk = torch_nn.Linear(part_dim, part_dim, bias=False)\n",
    "        self.Wv = torch_nn.Linear(part_dim, part_dim, bias=False)\n",
    "        # Linear mapping to return to the original \n",
    "        self.WO = torch_nn.Linear(heads * part_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Multi-headed self attention\n",
    "\n",
    "        Each head operates on a part of the embedding, i.e. we have q, k and v with shape\n",
    "        (batch_size, seq_length, heads, dim / heads)\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input with shape (batch_size, seq_length, dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, dim = x.shape\n",
    "        part_dim = dim // self.heads\n",
    "        x = x.reshape(batch_size, seq_length, self.heads, part_dim)\n",
    "        \n",
    "        keys = self.Wk(x)\n",
    "        queries = self.Wq(x)\n",
    "        values = self.Wv(x)\n",
    "        \n",
    "        keys = self._restructure_tensor(keys, batch_size, seq_length, part_dim)\n",
    "        queries = self._restructure_tensor(queries, batch_size, seq_length, part_dim)\n",
    "        values = self._restructure_tensor(values, batch_size, seq_length, part_dim)\n",
    "\n",
    "        weights = query_key_weights(queries, keys)\n",
    "\n",
    "        y_tilde = weighted_avg(values, weights)\n",
    "        y_tilde = (\n",
    "            y_tilde.transpose(2, 1)\n",
    "            .reshape(batch_size, self.heads, seq_length, part_dim)\n",
    "            .transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .reshape(batch_size, seq_length, part_dim * self.heads)\n",
    "        )\n",
    "        return self.WO(y_tilde)\n",
    "\n",
    "    def _restructure_tensor(self, x, batch_size, seq_length, part_dim):\n",
    "        \"\"\" Reshaping q, k and v tensors\n",
    "\n",
    "        For efficient vectorisation we stack the different heads in the batch_size dimension.\n",
    "        Think of it as temporarily expanding the batch_size with every head.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            x.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .reshape(batch_size * self.heads, seq_length, part_dim)\n",
    "            .transpose(2, 1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1a1cebeb15a37888ce9cf8c421a9270",
     "grade": false,
     "grade_id": "cell-c6938740fc6cf9db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# The transformer block\n",
    "\n",
    "The majority of the implementation complexity is actually in the `SelfAttention` module. The transformer block is rather straight forward, it is just like the one described in the video lectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e517abaace6db4a5af44c710024bf601",
     "grade": false,
     "grade_id": "cell-9b67db79adeb75d3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(torch_nn.Module):\n",
    "    \"\"\"Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention = SelfAttention(dim, heads)\n",
    "\n",
    "        self.normalization_1 = torch_nn.LayerNorm(dim)\n",
    "        self.normalization_2 = torch_nn.LayerNorm(dim)\n",
    "        \n",
    "        # The size of the hidden layer is a hyper-parameter,\n",
    "        # but the consensus is that it should at least be larger than the input/output size\n",
    "        self.feed_forward = torch_nn.Sequential(\n",
    "            torch_nn.Linear(dim, 4 * dim),\n",
    "            torch_nn.ReLU(),\n",
    "            torch_nn.Linear(4 * dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.self_attention(x)\n",
    "        # Note how the residual (skip) connections are implemented as simple addition.\n",
    "        x = self.normalization_1(x + y)\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        return self.normalization_2(fed_forward + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9800f2211ff3b5b80a0e2bae1fa04b4",
     "grade": false,
     "grade_id": "cell-a68000a248c4d4f7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now, we are done with the general module. To create an actual transformer yet we must choose an actual problem so that we can specify input, embedding and output.\n",
    "Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e16a26217bacc6bf649499dcd9bd0b4",
     "grade": false,
     "grade_id": "cell-5fcb1a036ccf47fc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 3. IMDB Classification\n",
    "\n",
    "Transformers are often very complex models. Whenever you see impressive transformer results they are likely produced with a transformer with many millions, if not billions, of parameters. We don't really have that computational budget for a part of a home assignment. Instead, we will show a classifying task that is reasonable but still not a toy example: classification of IMDB reviews. Even this small example takes a considerable time to train.\n",
    "\n",
    "The purpose is to build on the computer labs and to give you some inspiration for how to solve a general problem with `pytorch`. It will show you how to install additional python libraries (useful for the project) and some advice on how to construct a training/validation loop. We do not expect you to modify the code, **you don't even have to run it** if you feel that your cloud credits are starting to run low. However, you should read and understand the code, it will help you answer the questions at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e91acbf40690b9e9414cfcffe4e897d5",
     "grade": false,
     "grade_id": "cell-782a05ee9fec69cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The data\n",
    "\n",
    "The IMDB data is provided by an external python module called `torchtext`.\n",
    "You can add it to the dml conda environment with:\n",
    "```\n",
    "conda install -c pytorch torchtext\n",
    "```\n",
    "Make sure that you have activated the dml environment before your run it.\n",
    "\n",
    "Processing text data can be tedious and error prone. For prototyping it is nice to use some third-party library which has done most of the work for you. You do not really need to focus on the data processing here, since it will be different for every task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e2478c2155979c606309d025969e754",
     "grade": false,
     "grade_id": "cell-5554838bcc3faced",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is one the entire family will enjoy... even those who consider themselves too old for fairy tales. shelley duvall outdid herself with this unique, imaginative take on nearly all of the popular fairy tales of childhood. the scripts offer new twists on the age-old fables we grew up on and they feature a handful of stars in each episode. \"cinderella\" is no exception to duvall's standard and in my opinion it's one of the top five of the series, highlighted by jennifer beals (remember her from \"flashdance\"--and she's still in hollywood today making a movie here and there) in the title role, jean stapleton as the fairy godmother with a southern accent and eve arden as the embodiment of wicked stepmotherhood. edie mcclurg (\"ferris bueller's day off\") and jane alden make for a hilarious duo as the stepsisters. matthew broderick is an affable prince henry. you'll all keep coming back for this one!\n",
      "\n",
      "Label:  pos\n"
     ]
    }
   ],
   "source": [
    "from torchtext import data, datasets\n",
    "\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "def get_loaders(vocabulary_len, batch_size, device, split_ratio=0.8):\n",
    "    \"\"\"Load the IMDB data\"\"\"\n",
    "    tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    train, test = tdata.split(split_ratio)\n",
    "\n",
    "    TEXT.build_vocab(\n",
    "        # We have to leave space for two special tokens.\n",
    "        train, max_size=vocabulary_len - 2\n",
    "    )\n",
    "    LABEL.build_vocab(train)\n",
    "\n",
    "    train_loader, test_loader = data.BucketIterator.splits(\n",
    "        (train, test), batch_size=batch_size, device=device\n",
    "    )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def view_example_text(index):\n",
    "    \"\"\"Helper function to look at a sample.\n",
    "    \n",
    "    The dataset is quite slow to load. \n",
    "    \"\"\"\n",
    "    train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    sample_text = train[index].text\n",
    "    sample_label = train[index].label\n",
    "    # Simply print the list of words, separated by space.\n",
    "    print(\" \".join(sample_text))\n",
    "    print(\"\\nLabel: \", sample_label)\n",
    "\n",
    "view_example_text(118)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23e7fe517ddbd840d59b0863cd1c75ca",
     "grade": false,
     "grade_id": "cell-d1ffc03b003a1689",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The transformer\n",
    "We will create a simple transformer that takes as input text in the form of a python list of words and which outputs a  probability vector over the two classes \"pos\" and \"neg\" (technically, the output will be the input to a log-softmax).\n",
    "We make the simplest (and less memory efficient) version of position embedding as described in the video lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1595f37785d61071e5731927850a415",
     "grade": false,
     "grade_id": "cell-ff83666a724761f0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(torch_nn.Module):\n",
    "    def __init__(self, dim, heads, depth, seq_length, num_tokens, num_classes, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.num_tokens = num_tokens\n",
    "        \n",
    "        self.pos_emb = torch_nn.Embedding(seq_length, dim)\n",
    "        self.token_emb = torch_nn.Embedding(num_tokens, dim)\n",
    "\n",
    "        transformer_blocks = []\n",
    "        for _ in range(depth):\n",
    "            transformer_blocks.append(TransformerBlock(dim=dim, heads=heads))\n",
    "\n",
    "        # The Sequential wrapper is convenient when you want to repeat similar blocks.\n",
    "        # A down-side is that it is harder access intermediate values for debugging.\n",
    "        self.transformer_blocks = torch_nn.Sequential(*transformer_blocks)\n",
    "\n",
    "        # The last part is problem specific. Here we want to map our transformer embeddings\n",
    "        # to a probability distribution.\n",
    "        # We will use a linear layer to produce log logits (the input to a  log-softmax function).\n",
    "        self.output_map = torch_nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Transformer forward method\n",
    "\n",
    "        Args:\n",
    "            x Tensor(batch_size, seq_length): Word indices representing sequence of words.\n",
    "        Returns:\n",
    "            Tensor(batch_size, num_classes): Log logits\n",
    "        \"\"\"\n",
    "        tokens = self.token_emb(x)\n",
    "        batch_size, seq_length, dim = tokens.size()\n",
    "\n",
    "        # Note that we create a completely new tensor which must be moved to the proper device.\n",
    "        # This is why we must store the device in self.device.\n",
    "        pos = torch.arange(seq_length, device=self.device)\n",
    "        pos = self.pos_emb(pos)[None, :, :].expand(batch_size, seq_length, dim)\n",
    "\n",
    "        x = tokens + pos\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        x = self.output_map(x.mean(dim=1))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26237d0c1cce97be356e04085a87fc75",
     "grade": false,
     "grade_id": "cell-957262dc380f1c7c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now, for the train./val loop. This can be written in many ways but based on common misstakes in HA1, hints might be in order:\n",
    "\n",
    "- Separate your code into smaller pieces, i.e. functions. It makes it easier to find bugs and easier to reuse code.\n",
    "- Use separate functions to calculate metrics. If you want to calculate, say accuracy, during both training and validation, don't copy the code. Write one function and make sure that it works, then reuse it.\n",
    "- Adding measurements to a running metrics can be tricky. Below is a solution that is a bit overkill but that is okay, since it is hard to use it incorrectly.\n",
    "\n",
    "Note 1: the code below can be modified so that you can play around with it.\n",
    "\n",
    "Note 2: timing this on Azure, a single epoch took ~5 min. Feel free to reduce the number of epochs or just study the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d76c90279076a8b6b6db7db8fd6a8419",
     "grade": true,
     "grade_id": "cell-db83c03e65ea6f71",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Epoch: 1/5: time: 315.6, train loss: 0.695, train acc: 0.525, val. loss 0.679, val. acc: 0.533\n",
      "Epoch: 2/5: time: 311.8, train loss: 0.604, train acc: 0.669, val. loss 0.592, val. acc: 0.735\n",
      "Epoch: 3/5: time: 306.3, train loss: 0.502, train acc: 0.757, val. loss 0.523, val. acc: 0.765\n",
      "Epoch: 4/5: time: 307.7, train loss: 0.432, train acc: 0.802, val. loss 0.464, val. acc: 0.807\n",
      "Epoch: 5/5: time: 304.0, train loss: 0.384, train acc: 0.828, val. loss 0.443, val. acc: 0.822\n",
      "You have now trained a transformer!\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, max_seq_len):\n",
    "    \"\"\"Train epoch\"\"\"\n",
    "    train_loss = AccumulatingMetric()\n",
    "    train_acc = AccumulatingMetric()\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_, label = batch.text[0], batch.label - 1\n",
    "\n",
    "        input_ = _truncate_input(input_, max_seq_len)\n",
    "        pred = model(input_)\n",
    "        loss = F.nll_loss(pred, label)\n",
    "        loss.backward()\n",
    "        train_loss.add(loss.item())\n",
    "\n",
    "        train_acc.add(accuracy(pred, label))\n",
    "\n",
    "        # Gradient clipping is a way to ensure\n",
    "        # torch_nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return train_loss.avg(), train_acc.avg()\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, max_seq_len):\n",
    "    val_loss = AccumulatingMetric()\n",
    "    val_acc = AccumulatingMetric()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_, label = batch.text[0], batch.label - 1\n",
    "\n",
    "            input_ = _truncate_input(input_, max_seq_len)\n",
    "            pred = model(input_)\n",
    "            val_loss.add(F.nll_loss(pred, label).item())\n",
    "\n",
    "            val_acc.add(accuracy(pred, label))\n",
    "\n",
    "    return val_loss.avg(), val_acc.avg()  # TODO: loss\n",
    "\n",
    "\n",
    "def accuracy(pred, label):\n",
    "    hard_pred = pred.argmax(1)\n",
    "    return (hard_pred == label).float().mean().item()\n",
    "\n",
    "\n",
    "def _truncate_input(input_, max_seq_len):\n",
    "    if input_.size(1) > max_seq_len:\n",
    "        input_ = input_[:, :max_seq_len]\n",
    "    return input_\n",
    "\n",
    "\n",
    "class AccumulatingMetric:\n",
    "    \"\"\"Accumulate samples of a metric and automatically keep track of the number of samples.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metric = 0.0\n",
    "        self.counter = 0\n",
    "\n",
    "    def add(self, value):\n",
    "        self.metric += value\n",
    "        self.counter += 1\n",
    "        \n",
    "    def avg(self):\n",
    "        return self.metric / self.counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_tokens = 50_000\n",
    "max_length = 512\n",
    "embedding_size = 128\n",
    "num_heads = 8\n",
    "num_classes = 2\n",
    "depth = 6\n",
    "\n",
    "model = Transformer(\n",
    "    dim=embedding_size,\n",
    "    heads=num_heads,\n",
    "    depth=depth,\n",
    "    seq_length=max_length,\n",
    "    num_tokens=num_tokens,\n",
    "    num_classes=num_classes,\n",
    "    device=device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "lr_warmup = 1e4\n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "\n",
    "train_loader, test_loader = get_loaders(num_tokens, batch_size, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "# A scheduler is a principled way of controlling (often decreasing) the learning rate as time progresses.\n",
    "# Read more: https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lambda i: min(i / (lr_warmup / batch_size), 1.0)\n",
    ")\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start = time()\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, max_length)\n",
    "    val_loss, val_acc = validate_epoch(model, test_loader, max_length)\n",
    "    end = time()\n",
    "    print(\n",
    "        \"Epoch: {}/{}: time: {:.1f}, train loss: {:.3f}, train acc: {:.3f}, val. loss {:.3f}, val. acc: {:.3f}\".format(\n",
    "            epoch, num_epochs, end - start, train_loss, train_acc, val_loss, val_acc\n",
    "        )\n",
    "    )\n",
    "print(\"You have now trained a transformer!\")\n",
    "\n",
    "# I'm adding a ''# YOUR CODE HERE' tag so that the code above is not hidden when the assignment is generated.\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
